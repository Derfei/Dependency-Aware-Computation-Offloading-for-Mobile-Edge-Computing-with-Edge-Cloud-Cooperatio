# distribut
-- ed-deep-learning/IoT

分布式神经网络在嵌入式设备上需要运行的代码

## Bug 日志
### 2018-12-07
在IoT端的Server中有以下的错误
* 在dojob接口中，nexttask被用错了，nexttask是交给我执行的任务的nexttask，而不是该任务的前置任务
* 在dojob接口中，formertask是前面完成任务的id，而不是需要完成任务的formertasklist
* 在dojob接口中，对于最后一个任务的处理需要注意。


### 2018-12-08
在Iot端上的错误仔细思考发现有一些是没有错的
* 在dojob接口中，nexttask并没有被用错

### 2018-12-14
今天最终的任务结果已经跑出来来了 但是发现当初设计中存在的问题，第一个是
等待前置任务的时候任务完成时间没有记录的问题，所以在当有多个前置任务，获得前置
任务的过程过程中我们需要将前置任务的任务执行时间也是需要合并的，并不是只合并
输入数据。第二个就是，使用http服务器虽然可以简化网络层的设计，但是同样也存在
问题，问题就是数据发送方必须等待服务器将结果返回才能发送下个任务，这个过程中会有
等待的时延，显然这个时延试不必要的，所以解决办法是，将返回结果没有意义的函数直接
抽象出来，然后使用多线程进行执行，That is a great idea~~~


总的来说主要的Bug分为两个：
* 前置任务的完成时间的合并问题
* 因为Http请求导致等待的问题，我们需要一个异步信息机制，也就是我们根本不需要等待
返回的请求，或者说我们对返回结果没有时间要求


### 2018-12-15
下面来说主要进行两个方面的测试：第一个方面就是使用keras，使用的是不同层，使用现有的
网络模型进行测试，然后做出可视化界面，看看时间上的问题，进行offloading加速的速
率问题。下面一步就是找一个dependcy更加强的模型进行应用，查看offloading的效果，
刚刚突然想到一个idea就是在多个个设备的情况，如何要对这两个数据进行协同的话，
我们如何在offloading的时候同时考虑数据的协同，这是一个问题。总的来说接下来的工作
安排是这样的：
* 第一步： 使用线性的模型，做出可视化的界面
* 第二步： 使用dependency的模型，进行测试
* 第三步： 思考多设备情况下的系统表现

## 思考
### 2018-12-07
仔细思考，其实IoT端的程序，Edge端的程序，远程云端的程序在代码上其实没有什么区别，
三者的代码应该是可以通用的，但是还需要进行下一个深入的测试，可能当初在写
某些函数的时候想着三者代码会不一样，所以异常处理可能有些问题。
Iot端的程序主要是用来生成应用，进行调度的主题。突然想到一问题就是：
* 调度角色在不同的设备上进行运行的时候，系统会有怎样的表现
* 应用的生成不一定全是在IoT端，也可以在Edge端，也可以在云端。一般来说数据的采
者往往也是应用的产生者，IoT可以成为数据的来源，但是其他设备也是可以成为数据的
来源，我们可以假设有这样一种情形： Iot应用的数据来源不仅需要来自本省产生的数据，
也需要来自Edge的数据，那我们应该如何进行调度？
* 数据流的调度和计算能力的调度是否可以进行分离 两者之间是否可以产生
某种联系，某种关系

### 2018-12-14
仔细思考，如果客户端对服务端的请求返回的结果并没有直接的动作响应，或者说对于服务端
的请求结果返回我们可以等待更多的时间，那我们该如何编写请求处理？
* 第一种办法是使用多线程，将请求处理放到线程当中
* 第二种办法就是使用异步信息机制，这个是我们目前还没有用过的，I need a try!!!


### 2018-12-15
对于上面的异步信息机制，已经使用多线程得到解决，每次发送任务的时候，对于不需要
及时获得信息的任务使用线程的方式进行发送，做到异步信息处理，这样不必等待请回回答
就可以执行下一步的操作。

对于多设备，多数据情况下的offloading问题，如何根据需要多数据协同设计一个比较
适用的offloading算法，这是一个问题

### 2019-06-04
如何解决tensorflow同时定义任务图的会话问题？
* 在初始化的时候生成任务图，在执行过程中再加载权重： 此方法对于一些参数比较多的任务图来说是有用的，
但是对于本身图比较大的任务图是无用的，因为在初始的过程中还是要对任务图进行生成，还是会占用内存空间

* 使用多个会话，在加载权重过程中，使用tensorflow生成一个会话，然后执行任务图
* 使用动态图机制，
* 每次只允许一个深度学习任务进行执行，只有等完成之后才能开始下一个深度学习任务图
* 将树莓派设备迁移到Jeston上面进行

### 2019-06-05
* 查看是否存在可能性压缩内存，将VGG放在本地执行
> 通过观察发现可能是全连接层的参数过多，计算的时候需要全部加载在内存当中，如果将两个全连接层
放在一起的话，会使内存爆炸，所以将最后一层进行拆分，分成多层。
* 如果没有这种可能，则只能进行设备迁移，将jestion帮在小车上，通过jestion进行任务的执行。

### 2019-06-06 
#### 编码心得
* 对于keras来说，任务图的初始化必须在同一个函数当中，，如果在不同的函数则当
两个函数同时生成任务图的时候，会出现会话丧失的情况，因为tensorflow的后端是使用
一个会话的的
* 当任务图过大的时候，占用内存过多，也不能在同一个函数中进行初始化，如果内存过多，
会被放到虚存当中，当从虚存中拿出的时候，在loadweight的时候，会抛出Tensorflow不是
来自同一个任务图的异常


### 2019-06-06 
####编码心得
* h5读写错误，一般是临时文件保存的模型和用户文件存储的模型文件两者对不上
可以手动将模型进行复制，可能是其中的io流错误导致

*Tensor("Placeholder:0", shape=(7, 7, 3, 64), dtype=float32) 
must be from the same graph as Tensor("conv1/kernel:0", 
shape=(7, 7, 3, 64), dtype=float32_ref).


### 2019-06-07
#### Bug日志
发现本地执行完任务之后，没有及时发送任务给边缘和远程，但是过了一段时间之后就开始发送了

### 2019-06-08 
### Bug日志
发现在任务运行过程中，存在一些任务完成，但是一些任务没有完成的情况了，
任务完成说明整个执行流程是同的
但是一些任务没有完成，异常为数据输入格式对不上，有点怪
但是一些任务没有完成，异常为数据输入格式对不上，有点怪

> Bug原因为： python中的queue只有等填充满之后才会进行阻塞，其他情况下不会进行阻塞，到时两个进程
可能同时取到一个任务，这样将任务同时写入同一个文件当中，导致会有任务数据变为以前的两倍.


#### 解决办法


### 2019-06-09
#### 无线网卡安装教程
* 连接地址： https://github.com/abperiasamy/rtl8812AU_8821AU_linux
* 网卡型号： 8812au

* 连接地址： https://github.com/whitebatman2/rtl8821CU.git
* 网卡型号： 8821cu

#### 试验参数记录

#### 轨迹一
* 1--2 用时 10.13
* 2--3 用时 12.73

### 2019-06-11
#### 编码日志
今天解决了多线程同时读取数据的问题，同时发现网络有时候会掉线，这是有事同时执行
多个任务，导致服务暂时卡死，无法正常连接

同时服务端要安装Opensll，这样可以保持长连接

### 2019-06-11
### 编码日志
将离线算法结合到offline平台完成

下一步需要做的地方：
* 对VGG数据重新进行测量 对workload数据进行重新测量
* 将VGG新数据更新到离线算法
* 设置GCC的任务队列数量考虑
* 进行真实平台的实验

### 2019-09-07
### 编码日志
工作保存
* 今天已经写到了如何计算能耗方面，对于通讯部分的能耗计算还在coding
* 还在完善应用完成之后的应用完成时间，应用完成能耗 应用完成时间列表的函数
* 还在晚上实验一部分的功能